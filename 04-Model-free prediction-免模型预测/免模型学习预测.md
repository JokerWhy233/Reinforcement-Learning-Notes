# 强化学习—免模型预测

> 作者：YJLAugus  博客： https://www.cnblogs.com/yjlaugus

## 前言

在第二章[强化学习-动态规划-DP](https://www.yuque.com/yjlaugus/reinforcement-learning-notes/wfea35)中，我们讨论了用动态规划来求解强化学习预测问题和控制问题的方法。但是由于动态规划法需要在每一次回溯更新某一个状态的价值时，回溯到该状态的所有可能的后续状态。导致对于复杂问题计算量很大。同时很多时候，我们连环境的**动态特性** $P$ 都无法知道，这时动态规划法根本没法使用。这时候我们如何求解强化学习问题呢？

从本章开始将花连续两讲的时间讨论解决一个可以被认为是MDP、但却不掌握MDP具体细节的问题，也就是讲述**如何直接从Agent与环境的交互来得得到一个估计的最优价值函数和最优策略**。这部分内容同样分为两部分，第一部分也就是本章的内容，聚焦于策略评估，也就是预测，直白的说就是在给定的策略同时不清楚MDP细节的情况下，估计Agent会得到怎样的最终奖励。下一讲将利用本章的主要观念来进行控制进而找出最优策略，最大化Agent的奖励。

本章内容分为三个小部分，分别是**蒙特卡洛强化学习**、**时序差分强化学**习和介于两者之间的**λ时序差分强化学习**。相信读者在阅读本讲内容后会对这三类学习算法有一定的理解。

其中在第三章[**蒙特卡洛（Markov Chain & Monte Carlo, MCMC）方法**](https://www.yuque.com/yjlaugus/reinforcement-learning-notes/gw7d7v) 对蒙特卡洛方法进行了一个简单的介绍，这样对于这一章节问题的解决会有很多帮助。

## 免模型的强化学习问题定义

在动态规划法中，强化学习的两个问题是这样定义的：

* 预测问题，即给定强化学习的6个要素：状态集 $S$, 动作集 $A$, 模型状态转化概率矩阵$P$, 即时奖励 $R$，衰减因子$\gamma$, 给定策略 $\pi$，求解该策略的状态价值函数 $v_{π}$。

* 控制问题，也就是求解最优的价值函数和策略。给定强化学习的5个要素：状态集 $S$, 动作集 $A$, 模型状态转化概率矩阵$P$, 即时奖励 $R$，衰减因子$\gamma$, 给定策略 $\pi$， 求解最优的状态价值函数$v_*$和最优策略$\pi_*$　

可见，**模型状态转化概率矩阵$P$始终是已知的，即MDP已知**，对于这样的强化学习问题，我们一般称为**基于模型的强化学习问题**。

不过有很多强化学习问题，我们没有办法事先得到模型状态转化概率矩阵$P$，这时如果仍然需要我们求解强化学习问题，那么这就是不基于模型的强化学习问题了——**免模型的强化学习**。它的两个问题一般的定义是：　　　　

* 预测问题，即给定强化学习的5个要素：状态集$S$, 动作集$A$ 即时奖励$R$，衰减因子$\gamma$，给定策略$\pi$， 求解该策略的状态价值函数$v_\pi$

* 控制问题，也就是求解最优价值函数和策略。给定强化学习的5个要素：状态集$S$, 动作集$A$, 即时奖励$R$，衰减因子$\gamma$, **探索率 $\epsilon$** , 求解最优的动作价值函数$q_*$和最优策略$\pi_*$　

本章节要讨论的蒙特卡洛方法就是上述免模型的强化学习问题。

## 蒙特卡洛强化学习 (Monte-Carlo Reinforcement Learning)

### 蒙特卡洛强化学习概念

蒙特卡洛强化学习：是在不清楚MDP状态转移及即时奖励的情况下，直接从经历完整的`Episode`来学习状态价值，通常情况下某状态的价值等于在多个Episode中以该状态算得到的所有收获的平均。

> Episode ：agent根据某个策略执行一系列action到结束就是一个episode。

注：**收获**不是针对Episode的，它存在于Episode内，针对于Episode中某一个状态。从这个状态开始经历完Episode时得到的有衰减的即时奖励的总和。从一个Episode中，我们可以得到该Episode内所有状态的收获。当一个状态在Episode内出现多次，该状态的收获有不同的计算方法，下文会讲到。

**完整的Episode** 指必须从某一个状态开始，Agent与Environment交互直到**终止状态**，环境给出终止状态的即时收获为止。

### 蒙特卡洛强化学习特点

蒙特卡洛强化学习有如下特点：不基于模型本身，直接从经历过的Episode中学习，必须是**完整的Episode**，使用的思想就是用平均收获值代替价值。理论上Episode越多，结果越准确。

蒙特卡罗法通过采样若干经历完整的`状态序列(episode)`来估计状态的真实价值。所谓的经历完整，就是这个序列必须是达到终点的。比如下棋问题分出输赢，驾车问题成功到达终点或者失败。有了很多组这样经历完整的状态序列，我们就可以来近似的估计状态价值，进而求解预测和控制问题了。

### 蒙特卡洛策略评估 (Monte-Carlo Policy Evaluation)

> **目标：**在给定策略下，从一系列的完整Episode经历中学习,最后求得到该策略下的状态价值函数。

在解决问题过程中主要使用的信息是一系列完整Episode。其包含的信息有：状态的转移、使用的行为序列、中间状态获得的即时奖励以及到达终止状态时获得的即时奖励。其特点是使用有限的、完整Episode产生的这些经验性信息经验性地推导出每个状态的平均收获，以此来替代收获的期望，而后者就是状态价值。通常需要掌握完整的MDP信息才能准确计算得到。

数学描述如下：

基于特定策略 $\pi$ 的一个Episode信息可以表示为如下的一个序列：
$$
\large S_1,A_1,R_2,...,S_k\backsim\pi
$$



$t$ 时刻，$S_t$ 的收获：
$$
\large G_t = R_{t+1}+\gamma R_{t+2}+...+\gamma ^{T-1}R_T
$$


其中，$T$ 为终止时刻。

该策略下某一状态 $s$ 的价值：
$$
\large v_\pi(s) = E_\pi[G_t \rvert S_t = s]
$$

> 注： $R_{t+1}$ 表示的是 $t$时刻agent在状态 $S_t$ 获得的即时奖励，下文都使用这种下标来表示即时奖励。更准确的表述为：个体在状态 $S_t$ 执行一个行为$a$ 后离开该状态获得的即时奖励。

很多时候，即时奖励只出现在Episode结束状态时，但不能否认在中间状态也可能有即时奖励。公式里的 $R_t$ 指的是**任何状态**得到的即时奖励，这一点尤其要注意。

在状态转移过程中，可能发生一个状态经过一定的转移后又一次或多次返回该状态，此时在一个Episode里如何计算这个状态发生的次数和计算该Episode的收获呢？可以有如下两种方法：

#### 首次访问蒙特卡洛策略评估

在给定一个策略，使用一系列完整Episode评估某一个状态s时，对于每一个Episode，仅当该状态**第一次**出现在一个 episode中时：

* 状态出现的次数加1 ：$N(s) \leftarrow N(s) + 1$
* 总的收获更新：$S(s) \leftarrow S(s) + G_t$
* 状态 s 的价值：$V(s) = S(s)/N(s)$

当 $N(s) \rightarrow \infty$ 时，$V(s)\rightarrow v_\pi(s)$

#### 每次访问蒙特卡洛策略评估

在给定一个策略，使用一系列完整Episode评估某一个状态s时，对于每一个Episode，状态 s **每次**出现在一个epospde中时：

* 状态出现的次数加1 ：$N(s) \leftarrow N(s) + 1$
* 总的收获更新：$S(s) \leftarrow S(s) + G_t$
* 状态 s 的价值：$V(s) = S(s)/N(s)$

当 $N(s) \rightarrow \infty$ 时，$V(s)\rightarrow v_\pi(s)$。计算的公式与 **首次访问蒙特卡洛策略评估** 的公式相同，但是具体的意义却不同，下一以一个简单的例子进行说明。

**二十一点** 二十一点又名黑杰克（Blackjack），是一种流行于赌场的游戏，其目标是使得你的扑克牌点数之和不超过21的情况下越大越好。K、Q、J和10牌都算作10点（一般记作T，即ten之意）；A 牌（ace）既可算作1点也可算作11点，由玩家自己决定（当玩家停牌时，点数一律视为最大而尽量不爆，如A+9为20，A+4+8为13，A+3+A视为15）。游戏开始时，会给玩家和庄家各发两张牌。庄家的牌一张正面朝上，一张背面朝上，玩家两张都是明牌（都正面朝上）如果玩家的两张牌分别是一张A，一张10点（可能是10，J， Q，K），这种情况称为**天和**，玩家直接获胜。除非庄家也是天和，那就是平局。如果玩家不是天和，那么他可以一张一张地继续要牌，直到他主动停止（停牌）或者牌的点数和超过21点（爆牌）。如果玩家选择停牌，就轮到庄家行动。庄家根据一个固定的策略进行游戏：他一直要牌，直到点数等于或超过17时停牌。如果庄家爆牌，那么玩家获胜，否则根据谁的点数更靠近21决定胜负或者平局。

![](https://gitee.com/YJLAugus/pic-go/raw/master/img/blackj.jpg)

根据以上游戏规则，我们得到如下信息：

**状态空间：** （多达200种，根据对状态的定义可以有不同的状态空间，这里采用的定义是牌的分数，不包括牌型）

* 当前牌的分数（12 - 21），低于12时，你可以安全的再叫牌，所以没意义。
* 庄家出示的牌（A - 10），庄家会显示一张牌面给玩家，另一张为暗牌。
* 我有“useable” ace吗？（是或否）A既可以当1点也可以当11点。

**行为空间：**

* 停止要牌 stick
* 继续要牌 twist

**奖励（停止要牌）：**

* +1：如果你的牌分数大于庄家分数
*   0： 如果两者分数相同
* -1：如果你的牌分数小于庄家分数

**奖励（继续要牌）：**

* -1：如果牌的分数>21，并且进入终止状态
*  0：其它情况

**状态转换（Transitions）**：如果牌分小于12时，自动要牌

**当前策略**：牌分只要小于20就继续要牌。

**求解问题：**评估该策略的好坏。

**求解过程：**使用庄家显示的牌面值、玩家当前牌面总分值来确定一个二维状态空间，区分手中有无A分别处理。统计每一牌局下决定状态的庄家和玩家牌面的状态数据，同时计算其最终收获。通过模拟多次牌局，计算每一个状态下的平均值，得到如下图示。

![image-20201231132127235](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20201231132127235.png)

**最终结果：**无论玩家手中是否有A牌，该策略在绝大多数情况下各状态价值都较低，只有在玩家拿到21分时状态价值有一个明显的提升。

这个例子只是使读者对蒙特卡洛策略评估方法有一个直观的认识。

为了尽可能使读者对MC方法有一个直接的认识，我们尝试模拟多个二十一点游戏牌局信息，假设我们仅研究初始状态下庄家一张明牌为4，玩家手中前两张牌和为15的情形，不考虑A牌。在给定策略下，玩家势必继续要牌，根据蒙特卡洛策略评估则可能会出现如下多种情形：

| 庄家总序列                | 玩家最终序列             | 玩家获得奖励 | 当前估计的状态价值 |
| ------------------------- | ------------------------ | ------------ | ------------------ |
| 4，10，3 （17点）         | Q，5，5（20点）          | +1           | +1                 |
| 4，J，7（21点）           | 9，6，8（23点，爆牌）    | -1           | 0                  |
| 4，10，2，8（24点，爆牌） | 7，8，6（21点）          | +1           | 0.333              |
| 4，4，2，7（17点）        | J，5，Q（15点）          | -1           | 0                  |
| 4，9（13点）              | 5，K，4，8（27点，爆牌） | -1           | -0.2               |
| 4，3，K（17点）           | 8，7，5（20点）          | +1           | 0                  |
| ...                       | ...                      | ...          | ...                |

以上状态价值由 以下公式得出：

* 状态出现的次数加1 ：$N(s) \leftarrow N(s) + 1$
* 总的收获更新：$S(s) \leftarrow S(s) + G_t$
* 状态 s 的价值：$V(s) = S(s)/N(s)$

以前三个序列为例

1. 序列一： N = 1， S = 1，V = 1 / 1 = 1;
2. 序列二： N = 2， S = 1 - 1 = 0，V = 0 / 2 = 0;
3. 序列三： N = 3， S = 0 + 1 = 1，V = 1 / 3 = 0.333

可以看到，使用只有当牌不小于20的时候才停止叫牌这个策略（注：庄家不需要遵从这个策略），**前6次平均价值为0**，如果玩的牌局足够多，按照这样的方法可以针对每一个状态（庄家第一张明牌，玩家手中前两张牌分值合计）都可以制作这样一张表，进而计算玩家奖励的平均值。通过结果，可以发现这个策略并不能带来很高的玩家奖励。

这里给出表中第一个对局对应的信息序列（Episode）：
$$
\large S_0<4,15>, A_0<要牌>,R_1<0>,S_1<4,20>,A_1<停止要牌>，R_2<+1>
$$
可以看出，这个完整的Episode中包含两个状态，其中第一个状态的即时奖励为0，后一个状态是终止状态，根据规则，玩家赢得对局，获得终止状态的即时奖励+1。读者可以加深对即时奖励、完整Episode的理解。

通过上面的例子，我们使用蒙特卡洛方法求解的是**平均收获**，根据上面的例子，可以很清楚的知道这点。通常计算平均值就需要预先存储所有的数据，最后加和取平均，这样真的在计算机中计算时，会浪费很多空间。下面介绍一种更简单的方法——**累进更新平均值（Incremental Mean）**

### 累进更新平均值（Incremental Mean）

这里提到了在实际操作时常用的一个实时更新均值的办法，使得在计算平均收获时不需要存储所有既往收获，而是每得到一次收获，就计算其平均收获。

理论公式如下：
$$
\large{
\begin{align}
\mu_k =& \frac{1}{k}\sum_{j=1}^k x_j \\
=& \frac{1}{k}\left(x_k + \sum_{j=1}^{k-1} x_j \right) \\
=& \frac{1}{k}(x_k+(k-1)\mu_{k-1}) \\
=& \frac{1}{k}x_k+(1-\frac{1}{k})\mu_{k-1}\\
=& \mu_{k-1} + \frac{1}{k}(x_k-\mu_{k-1})

\end{align}
}
$$
这个公式比较简单。把这个方法应用于蒙特卡洛策略评估，就得到下面的蒙特卡洛累进更新。

### 蒙特卡洛累进更新

对于一些列 Episodes中的每一个： $ S_1,A_1,R_2,...,S_k\backsim\pi $，对于Episode里的每一个状态$S_t$ ，有一个收获$G_t$，每碰到一次$S_t$ ，使用下面的公式计算状态的平均价值$V(S_t)$：
$$
\large V(S_t) \leftarrow V(S_t)  + \frac{1}{N(S_t)}(G_t -  V(S_t) )
$$
其中：
$$
\large N(S_t) = N(S_t)  +1
$$
这样我们无论数据量是多还是少，算法需要的内存基本是固定的 。

有时候，尤其是海量数据做分布式迭代的时候，我们可能无法准确计算当前的次数$N(S_t)$，这时我们可以用一个系数$\alpha$来代替，即：
$$
\large V(S_t) = V(S_t)  + \alpha(G_t -  V(S_t) )
$$


以上就是蒙特卡罗方法求解预测问题的整个过程。由于蒙特卡洛学习方法有许多缺点（后文会细说），因此实际应用并不多。接下来介绍实际常用的时序差分学习方法。

## 时序差分学习 Temporal-Difference Learning

### 时序差分学习特点

时序差分学习简称TD学习，它的特点如下：和蒙特卡洛学习一样，它也从Episode学习，不需要了解模型本身；但是它可以学习**不完整**的Episode，通过自身的引导（bootstrapping），猜测Episode的结果，同时持续更新这个猜测。

### 时序差分策略评估

我们已经学过，在Monte-Carlo学习中，使用实际的收获（return）$G_t$来更新价值（Value）：
$$
\large V(S_t) = V(S_t)  + \alpha(G_t -  V(S_t) ) \tag1
$$
由第一章的收获公式(注：收获/回报都是$G_t$，以后两者不会特别区分)可得：
$$
\large
G_t = R_{t+1} + \gamma R_{t+2}+\gamma^2 R_{t+3} \ ...\ +\gamma^{T-t-1}R_T = \sum_{i=0}^{\infty}\gamma^i R_{t+i+1} \quad \quad \gamma\in[0,1],\quad (T\rightarrow\infty)
$$
如果用$G_t$ 来更新价值的话，就可以写成如下：
$$
\large
G_t = R_{t+1} + \gamma V{(S_t+1)} \tag2
$$
在TD学习中，算法在估计某一个状态的价值时，用的是离开该状态的即刻奖励 $R_{t+1}$ 与下一个状态 $S_{t+1}$的预估价值乘以衰减系数$\gamma$ 组成，这符合Bellman方程的描述，故把（2）式带入（1）式得：
$$
\large V(S_t) = V(S_t)  + \alpha(R_{t+1} + \gamma V{(S_t+1)} -  V(S_t) ) \tag3
$$






## 参考文献

https://zhuanlan.zhihu.com/p/28107168

https://www.cnblogs.com/pinard/p/9492980.html

书

ppt

https://baike.baidu.com/item/21点/5481683?fr=aladdin